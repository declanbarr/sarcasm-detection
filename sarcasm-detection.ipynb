{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON file to array of JSON objects\n",
    "with open('./data/Sarcasm_Headlines_Dataset.json', 'r') as f:\n",
    "    data = json.loads(\"[\" + f.read().replace(\"}\\n{\", \"},\\n{\") + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from array of JSON objects\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictor and target from DataFrame\n",
    "X = df['headline']\n",
    "y = df['is_sarcastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer and padder parameters\n",
    "num_words = 1000\n",
    "oov_token = '<OOV>'\n",
    "pad_type = 'pre'\n",
    "trunc_type = 'post'\n",
    "embedding_dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tokenizer on the training set\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training and test sets\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "maxlen = max([len(x) for x in X_train_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the training and test sets\n",
    "X_train_padded = pad_sequences(X_train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n",
    "X_test_padded = pad_sequences(X_test_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_padded[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_EMBED_INITIALIZER = hp.HParam('embeddings_initializer', hp.Discrete([\"Constant\",\n",
    "\"GlorotNormal\",\n",
    "\"GlorotUniform\",\n",
    "\"HeNormal\",\n",
    "\"HeUniform\",\n",
    "\"Identity\",\n",
    "\"Initializer\",\n",
    "\"LecunNormal\",\n",
    "\"LecunUniform\",\n",
    "\"Ones\",\n",
    "\"Orthogonal\",\n",
    "\"RandomNorma\",\n",
    "\"RandomUniform\",\n",
    "\"TruncatedNormal\",\n",
    "\"VarianceScaling\",\n",
    "\"Zeros\",\n",
    "\"constant\",\n",
    "\"glorot_normal\",\n",
    "\"glorot_uniform\",\n",
    "\"he_normal\",\n",
    "\"he_uniform\",\n",
    "\"identity\",\n",
    "\"lecun_normal\",\n",
    "\"lecun_uniform\",\n",
    "\"ones\",\n",
    "\"orthogonal\",\n",
    "\"random_normal\",\n",
    "\"random_uniform\",\n",
    "\"truncated_normal\",\n",
    "\"variance_scaling\",\n",
    "\"zeros\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HP_EMBEDDINGS_REGULARIZER = hp.HParam('l2 regularizer', hp.RealInterval(0.0, 1.0))\n",
    "\n",
    "HP_EMBEDDINGS_CONSTRAINT = hp.HParam('embeddings_constraint', hp.Discrete([\"MaxNorm\",\n",
    "\"MinMaxNorm\",\n",
    "\"NonNeg\",\n",
    "\"RadialConstraint\",\n",
    "\"UnitNorm\",\n",
    "\"max_norm\",\n",
    "\"min_max_norm\",\n",
    "\"non_neg\",\n",
    "\"radial_constraint\",\n",
    "\"unit_norm\"]))\n",
    "\n",
    "\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete([\"sgd\",\n",
    "    \"rmsprop\",\n",
    "    \"adam\",\n",
    "    \"adadelta\",\n",
    "    \"adagrad\",\n",
    "    \"adamax\",\n",
    "    \"nadam\",\n",
    "    \"ftrl\"]))\n",
    "\n",
    "\n",
    "\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete([\n",
    "    \"relu\",\n",
    "    \"sigmoid\",\n",
    "    \"softmax\",\n",
    "    \"softplus\",\n",
    "    \"softsign\",\n",
    "    \"tanh\",\n",
    "    \"selu\",\n",
    "    \"elu\",\n",
    "    \"exponential\"]))\n",
    "\n",
    "    \n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_EMBED_INITIALIZER,HP_EMBEDDINGS_REGULARIZER,HP_EMBEDDINGS_CONSTRAINT,HP_ACTIVATION,HP_OPTIMIZER],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_words, \n",
    "                        output_dim=embedding_dim, \n",
    "                        mask_zero=True, \n",
    "                        embeddings_initializer=hparams[HP_EMBED_INITIALIZER], \n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(hparams[HP_EMBEDDINGS_REGULARIZER]), \n",
    "                        embeddings_constraint=hparams[HP_EMBEDDINGS_CONSTRAINT]))\n",
    "    model.add(Bidirectional(GRU(8)))\n",
    "    model.add(Dense(8, activation=hparams[HP_ACTIVATION]))\n",
    "    model.add(Dense(1, 'sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer=hparams[HP_OPTIMIZER],metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train_padded, y_train.values.reshape(-1, 1), epochs=3)#, validation_data=(X_test_padded, y_test.values.reshape(-1, 1)))\n",
    "    _, accuracy = model.evaluate(X_test_padded, y_test.values.reshape(-1, 1))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        accuracy = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "\n",
    "for i in range(10):\n",
    "    hparams = {\n",
    "        HP_EMBED_INITIALIZER: np.random.choice(HP_EMBED_INITIALIZER.domain.values),\n",
    "        HP_EMBEDDINGS_REGULARIZER: np.random.uniform(HP_EMBEDDINGS_REGULARIZER.domain.min_value, HP_EMBEDDINGS_REGULARIZER.domain.max_value),\n",
    "        HP_EMBEDDINGS_CONSTRAINT: np.random.choice(HP_EMBEDDINGS_CONSTRAINT.domain.values),\n",
    "        HP_ACTIVATION: np.random.choice(HP_ACTIVATION.domain.values),\n",
    "        HP_OPTIMIZER: np.random.choice(HP_OPTIMIZER.domain.values),\n",
    "    }\n",
    "    \n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    \n",
    "    run('logs/hparam_tuning/' + run_name, hparams)\n",
    "    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each model for 10 epochs and save the history for each to enable choosing best model based on validation accuracy/loss\n",
    "\n",
    "#history = model.fit(X_train_padded, y_train.values.reshape(-1, 1), epochs=3, validation_data=(X_test_padded, y_test.values.reshape(-1, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
